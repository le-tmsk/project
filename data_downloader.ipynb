{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWIMPLACES \n",
    "Andrea Tláskalová & Lenka Tomášková"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, first part consists of a class which will download data about places where to swim from website www.swimplaces.com. Specifically the core data from its API and description from each page. Second part shows you statistics, visualization or a feature to find the closest place to swim from given location.\n",
    "\n",
    "-------- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA DOWNLOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are downloaded from a combination of API and each place's webpage and are saved to a csv file. \\\n",
    "The class is ---- initiated --- and it is sufficient only to call method mergeData() to get the file. \\\n",
    "The comments are inclueded in the Class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only disadvantage is the the process of parsing through a string to get the Description takes a really long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#how to access another than the first page\n",
    "response = requests.get('https://www.mapotic.com/api/v1/maps/2941/public-pois/?page=1&page_size=10&ordering=created&page=4')\n",
    "print(response)\n",
    "#response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swim():\n",
    "    '''\n",
    "    This Class will be used to download all data and merge them into one DataFrame.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        \"\"\" Data are saved to the Class. \"\"\"\n",
    "        \n",
    "        self.pages_json = self.downloadPages()\n",
    "        self.get_data = self.getData()\n",
    "        pass\n",
    "    def downloadPages(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Download data from API and save to a json file.\n",
    "           The response of the link to get the data from API changed its behaviour, so in case it will not work - raw_data.json from 14/09/2020 is available to use.\n",
    "           The parameter page_size was probably changed by the owner, thus in case, there is a backup.\n",
    "           \n",
    "        \"\"\"\n",
    "        \n",
    "        resp=[]\n",
    "        b= tqdm(list(range(1,334)), desc=\"Downloading JSON data from API\")  # zmenili page_size - musime pres vice stranek\n",
    "        for i in b:\n",
    "            response = requests.get('https://www.mapotic.com/api/v1/maps/2941/public-pois/?page=1&page_size=10&ordering=created&page='+ str(i) )\n",
    "            resp.append(response.json())\n",
    "#         with open('raw_data.json', 'w') as raw:   # uložit do classy, ale aby se to pokaždé nestahovalo... plus podmínku, kdy se má stáhnout\n",
    "#             json.dump(resp, raw) \n",
    "        return resp\n",
    "            \n",
    "    def getData(self):\n",
    "        \"\"\"\n",
    "        Method getData() will get the information from data from API.\n",
    "        \"\"\"\n",
    "        \n",
    "        preview = self.pages_json\n",
    "#         with open('raw_data_stare.json') as raw_data: \n",
    "#             preview = json.load(raw_data)\n",
    "        latitude = []    \n",
    "        longitude = []\n",
    "        ids = [] \n",
    "        names = [] \n",
    "#         types =[]\n",
    "        ratings = []\n",
    "        count_ratings = []\n",
    "        created = []\n",
    "        for page in preview: \n",
    "            for result in page['results']:\n",
    "                names.append(result['name'])\n",
    "                ids.append(result['id'])\n",
    "                latitude.append(result['point']['coordinates'][0])\n",
    "                longitude.append(result['point']['coordinates'][1])\n",
    "#                 if result['category']['name']['en'] is not None:\n",
    "#                     types.append(result['category']['name']['en'][0])  #z nějakého důvodu nefunguje - NoneType error\n",
    "                ratings.append(result['rating']['average'])\n",
    "                count_ratings.append(result['rating']['count'])\n",
    "                created.append(result['created'])\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'ID' : ids, \n",
    "            'Name' : names, \n",
    "#             'type' : types,\n",
    "            'Average rating' : ratings,\n",
    "            'Number of ratings' : count_ratings,\n",
    "            'Created' : created,\n",
    "            'Longitude' : longitude,\n",
    "            'Latitude' : latitude})\n",
    "#         return df \n",
    "        return df  #zde ukáže momentálně pár záznamů, jen pro kontrolu\n",
    "    \n",
    "    def getDescPlaces(self, idn, name):\n",
    "        \"\"\"\n",
    "        Method to get the soup (only the part we are interested in) from each specific website. ID and name (without diacritics) have to be entered.\n",
    "        \"\"\"\n",
    "        \n",
    "        name = unicodedata.normalize('NFKD', name).lower()\n",
    "        new_name = ''\n",
    "        for c in name:\n",
    "            if not unicodedata.combining(c):\n",
    "                new_name += c\n",
    "                \n",
    "        odkaz = 'https://www.swimplaces.com/'  + str(idn) + '-'+ new_name.replace(' ', '-') \n",
    "        r = requests.get(odkaz)\n",
    "        r.encoding = 'UTF-8'\n",
    "        soup = BeautifulSoup(r.text,'lxml').find('meta', {'name' : 'description'})['content'].split('|')\n",
    "        return soup\n",
    "    \n",
    "    def getAtt(self):\n",
    "        \"\"\"\n",
    "        Method that will download the information about attributes for each place with the help of getDescPlaces().\n",
    "        \"\"\"\n",
    "        value = None\n",
    "        def find_between(string, start, end):  #pomocná na hledání ve stringu\n",
    "            return (string.split(start))[1].split(end)[0]\n",
    "        \n",
    "        nazev =[]\n",
    "        att = []\n",
    "        for i, n in self.get_data.iterrows(): #self.getData().iterrows(): \n",
    "            nazev.append(self.getDescPlaces(n['ID'], n['Name'])[0])\n",
    "            try:\n",
    "                att.append(self.getDescPlaces(n['ID'], n['Name'])[1])  # tady už to někdy hodilo chybu, ze to nenašlo...\n",
    "            except:\n",
    "                att.append(value)\n",
    "            time.sleep(0.1)\n",
    "        ids_att = self.get_data['ID'].values #self.getData()['ID'].values #přidat ID, pro pozdější join\n",
    "        \n",
    "        desc = []\n",
    "        refresh = []\n",
    "        diving = []\n",
    "        entrances = []\n",
    "        access = []\n",
    "        nudists = []\n",
    "        for ii in tqdm(att, desc = 'Searching through the description for attributes:',position=0, leave=True): # tady zkusim pridat taky tqdm\n",
    "            if ii is not None:\n",
    "                if 'Description:' in ii:\n",
    "                    try:\n",
    "                        desc.append(find_between(ii, 'Description:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip()) # najde atribut a vrátí jeho obsah - snad nikde jinde není dvojtečka :D\n",
    "                    except:\n",
    "                        desc.append(value)\n",
    "                else:\n",
    "                    desc.append(value)\n",
    "                    \n",
    "                if 'Refreshment' in ii:\n",
    "                    try:\n",
    "                        refresh.append(find_between(ii, 'Refreshment:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip())\n",
    "                    except:\n",
    "                        refresh.append(value)\n",
    "                else:\n",
    "                    refresh.append(value) \n",
    "                    \n",
    "                if 'Diving' in ii:\n",
    "                    try:\n",
    "                        diving.append(find_between(ii, 'Diving:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip())\n",
    "                    except:\n",
    "                        diving.append(value)\n",
    "                else:\n",
    "                    diving.append(value)\n",
    "                    \n",
    "                if 'Accessibility/parking' in ii:\n",
    "                    try:\n",
    "                        access.append(find_between(ii, 'Accessibility/parking:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip()) \n",
    "                    except:\n",
    "                        access.append(value)                                                         \n",
    "                else:\n",
    "                    access.append(value)  \n",
    "                    \n",
    "                if 'Entrance' in ii:\n",
    "                    try:\n",
    "                        entrances.append(find_between(ii, 'Entrance:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip()) \n",
    "                    except:\n",
    "                        entrances.append(value)\n",
    "                else:\n",
    "                    entrances.append(value)\n",
    "                    \n",
    "                if 'Nudist beach' in ii:\n",
    "                    try:\n",
    "                        nudists.append(find_between(ii, 'Nudist beach:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip())\n",
    "                    except:\n",
    "                        nudists.append(value) \n",
    "                else: \n",
    "                    nudists.append(value)\n",
    "            else:\n",
    "                desc.append(value)\n",
    "                refresh.append(value)\n",
    "                diving.append(value)\n",
    "                access.append(value)\n",
    "                entrances.append(value)\n",
    "                nudists.append(value)\n",
    "                    \n",
    "        attributes = pd.DataFrame({\n",
    "            'id_a' : ids_att,\n",
    "            'nazev' :nazev,\n",
    "            'Description' : desc,\n",
    "            'Refreshment' : refresh,\n",
    "            'Diving' : diving,\n",
    "            'Entrance' : entrances,\n",
    "            'Accessibility and parking' :access,\n",
    "            'Nudist beach' : nudists\n",
    "        })\n",
    "        return attributes\n",
    "    def mergeData(self):\n",
    "        ''' \n",
    "        Finally, both sources are merged and everything is saved to csv file.\n",
    "        '''\n",
    "        output = self.get_data.merge(self.getAtt(), left_on = 'ID', right_on = 'id_a')\n",
    "#         output = output['ids', 'names'] # , 'avg_rating', 'count_ratings', 'created', 'logitude', 'latitude', 'description', 'refreshment', 'diving', 'entrance', 'accessibility_parking', 'nudist_beach']\n",
    "        output.to_csv('raw_data.csv', sep = ',')\n",
    "        return print('Data prepared in csv file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading JSON data from API: 100%|██████████| 333/333 [06:37<00:00,  1.19s/it]\n",
      "Searching through the description for attributes:: 100%|██████████| 3321/3321 [00:00<00:00, 111133.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared in csv file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "swim = Swim()\n",
    "swim.mergeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching through the description for attributes:: 100%|██████████| 20/20 [00:00<00:00, 4808.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared in csv file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
