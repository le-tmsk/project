{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA DOWNLOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are downloaded from a combination of API (https://www.mapotic.com/api/v1/maps/2941/public-pois/?ordering=created&page=1&page_size=10) and each place's webpage and are saved to a csv file. \\\n",
    "It is sufficient only to call method mergeData() from class Swim() to get the csv file. The final file includes ID of the place, name, date of creation, longitude and latitude, attributes (description, refreshment, good for diving or nudist, entrance fee and accesibility) and rating. \\\n",
    "The comments are inclueded in the Class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only disadvantage of this process is opening each webpage and going through a string to get the Description, which takes really long time. The usual time is about 1 hour. Data are valid as of 20/09/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#how to access another than the first page -> add another parameter for page\n",
    "response = requests.get('https://www.mapotic.com/api/v1/maps/2941/public-pois/?page=1&page_size=10&ordering=created&page=4')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swim():\n",
    "    '''\n",
    "    This Class will be used to download all data and merge them into one DataFrame.\n",
    "    Method MergeData() has to be used.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        \"\"\" Data are saved to the Class. \"\"\"\n",
    "        \n",
    "        self.pages_json = self.downloadPages()\n",
    "        self.get_data = self.getData()\n",
    "        pass\n",
    "    def downloadPages(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Downloading data from API and save it to a json file.\n",
    "           The response of the link to get the data from API changed its behaviour few days ago, so in case it will not work properly - raw_data.json from 14/09/2020 is available to use.\n",
    "           The parameter page_size was probably changed by the owner (parameter pagination does not work anymore), thus in case, there is a backup.\n",
    "           \n",
    "        \"\"\"\n",
    "        \n",
    "        resp=[]\n",
    "        b= tqdm(list(range(1,334)), desc=\"Downloading JSON data from API\")  \n",
    "        for i in b:\n",
    "            response = requests.get('https://www.mapotic.com/api/v1/maps/2941/public-pois/?page=1&page_size=10&ordering=created&page='+ str(i) )\n",
    "            resp.append(response.json())\n",
    "#         with open('raw_data.json', 'w') as raw:   # in case of bad response from API\n",
    "#             json.dump(resp, raw) \n",
    "        return resp\n",
    "            \n",
    "    def getData(self):\n",
    "        \"\"\"\n",
    "        Method getData() will parse json format from API.\n",
    "        \"\"\"\n",
    "        \n",
    "        preview = self.pages_json\n",
    "#         with open('raw_data_stare.json') as raw_data: # in case of bad response from API\n",
    "#             preview = json.load(raw_data)\n",
    "        latitude = []    \n",
    "        longitude = []\n",
    "        ids = [] \n",
    "        names = [] \n",
    "        ratings = []\n",
    "        count_ratings = []\n",
    "        created = []\n",
    "        for page in preview: \n",
    "            for result in page['results']:\n",
    "                names.append(result['name'])\n",
    "                ids.append(result['id'])\n",
    "                latitude.append(result['point']['coordinates'][0])\n",
    "                longitude.append(result['point']['coordinates'][1])\n",
    "                ratings.append(result['rating']['average'])\n",
    "                count_ratings.append(result['rating']['count'])\n",
    "                created.append(result['created'])\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'ID' : ids, \n",
    "            'Name' : names, \n",
    "            'Average rating' : ratings,\n",
    "            'Number of ratings' : count_ratings,\n",
    "            'Created' : created,\n",
    "            'Longitude' : longitude,\n",
    "            'Latitude' : latitude})\n",
    "        return df  \n",
    "    \n",
    "    def getDescPlaces(self, idn, name):\n",
    "        \"\"\"\n",
    "        Method to get the soup (only the part we are interested in) from each specific website. ID and name (without diacritics) have to be entered.\n",
    "        \"\"\"\n",
    "        \n",
    "        name = unicodedata.normalize('NFKD', name).lower()\n",
    "        new_name = ''\n",
    "        for c in name:\n",
    "            if not unicodedata.combining(c):\n",
    "                new_name += c\n",
    "                \n",
    "        odkaz = 'https://www.swimplaces.com/'  + str(idn) + '-'+ new_name.replace(' ', '-') \n",
    "        r = requests.get(odkaz)\n",
    "        r.encoding = 'UTF-8'\n",
    "        soup = BeautifulSoup(r.text,'lxml').find('meta', {'name' : 'description'})['content'].split('|')\n",
    "        return soup\n",
    "    \n",
    "    def getAtt(self):\n",
    "        \"\"\"\n",
    "        Method that will download the information about attributes for each place with the help of getDescPlaces().\n",
    "        \"\"\"\n",
    "        value = None\n",
    "        def find_between(string, start, end):  #additional function to get information from a string\n",
    "            return (string.split(start))[1].split(end)[0]\n",
    "        \n",
    "        nazev =[]\n",
    "        att = []\n",
    "        for i, n in self.get_data.iterrows(): # downloading a string from webpages where Description is saved\n",
    "            nazev.append(self.getDescPlaces(n['ID'], n['Name'])[0])\n",
    "            try:\n",
    "                att.append(self.getDescPlaces(n['ID'], n['Name'])[1])  \n",
    "            except:\n",
    "                att.append(value)\n",
    "            time.sleep(0.1)\n",
    "        ids_att = self.get_data['ID'].values \n",
    "        \n",
    "        desc = []\n",
    "        refresh = []\n",
    "        diving = []\n",
    "        entrances = []\n",
    "        access = []\n",
    "        nudists = []\n",
    "        \n",
    "        # looking for specific attributes in each string for each place\n",
    "        for ii in tqdm(att, desc = 'Searching through the description for attributes:',position=0, leave=True): \n",
    "            if ii is not None:\n",
    "                if 'Description:' in ii:\n",
    "                    try:\n",
    "                        desc.append(find_between(ii, 'Description:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip()) # assumption: if the creator used colon in description, we will have only a part of the description...\n",
    "                    except:\n",
    "                        desc.append(value)\n",
    "                else:\n",
    "                    desc.append(value)\n",
    "                    \n",
    "                if 'Refreshment' in ii:\n",
    "                    try:\n",
    "                        refresh.append(find_between(ii, 'Refreshment:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip())\n",
    "                    except:\n",
    "                        refresh.append(value)\n",
    "                else:\n",
    "                    refresh.append(value) \n",
    "                    \n",
    "                if 'Diving' in ii:\n",
    "                    try:\n",
    "                        diving.append(find_between(ii, 'Diving:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip())\n",
    "                    except:\n",
    "                        diving.append(value)\n",
    "                else:\n",
    "                    diving.append(value)\n",
    "                    \n",
    "                if 'Accessibility/parking' in ii:\n",
    "                    try:\n",
    "                        access.append(find_between(ii, 'Accessibility/parking:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip()) \n",
    "                    except:\n",
    "                        access.append(value)                                                         \n",
    "                else:\n",
    "                    access.append(value)  \n",
    "                    \n",
    "                if 'Entrance' in ii:\n",
    "                    try:\n",
    "                        entrances.append(find_between(ii, 'Entrance:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip()) \n",
    "                    except:\n",
    "                        entrances.append(value)\n",
    "                else:\n",
    "                    entrances.append(value)\n",
    "                    \n",
    "                if 'Nudist beach' in ii:\n",
    "                    try:\n",
    "                        nudists.append(find_between(ii, 'Nudist beach:', ':').rsplit(' ', 1)[0].rsplit(',', 1)[0].strip())\n",
    "                    except:\n",
    "                        nudists.append(value) \n",
    "                else: \n",
    "                    nudists.append(value)\n",
    "            else:\n",
    "                desc.append(value)\n",
    "                refresh.append(value)\n",
    "                diving.append(value)\n",
    "                access.append(value)\n",
    "                entrances.append(value)\n",
    "                nudists.append(value)\n",
    "                    \n",
    "        # Attributes will be prepared in dataframe:\n",
    "        attributes = pd.DataFrame({\n",
    "            'id_a' : ids_att,\n",
    "            'nazev' :nazev,\n",
    "            'Description' : desc,\n",
    "            'Refreshment' : refresh,\n",
    "            'Diving' : diving,\n",
    "            'Entrance' : entrances,\n",
    "            'Accessibility and parking' :access,\n",
    "            'Nudist beach' : nudists\n",
    "        })\n",
    "        return attributes\n",
    "    def mergeData(self):\n",
    "        ''' \n",
    "        Finally, both data sources are merged and everything is saved to csv file.\n",
    "        '''\n",
    "        output = self.get_data.merge(self.getAtt(), left_on = 'ID', right_on = 'id_a')\n",
    "        output.to_csv('raw_data.csv', sep = ',')\n",
    "        return print('Data prepared in csv file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading JSON data from API: 100%|██████████| 333/333 [06:37<00:00,  1.19s/it]\n",
      "Searching through the description for attributes:: 100%|██████████| 3321/3321 [00:00<00:00, 111133.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared in csv file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "swim = Swim()\n",
    "swim.mergeData()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
